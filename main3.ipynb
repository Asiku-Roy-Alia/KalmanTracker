{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sort\n",
    "from utils import *\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "YOLO-v3 based object detector. This YOLO-v3 is pretrained on MS-COCO dataset.\n",
    "\"\"\"\n",
    "network = cv2.dnn.readNet(\"yolo/weights/yolov3.weights\",\"yolo/cfg/yolov3.cfg\") #  \"yolo/cfg/coco.data\"\n",
    "\n",
    "\n",
    "def detect(img):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  img: PIL Input Image\n",
    "  category: category of the object to filter(should be one of the categories from MS-COCO dataset)\n",
    "\n",
    "  Returns\n",
    "  ---------- \n",
    "  detections: List of detections. Each detection is a tuple of form (object_name, score, bbox).\n",
    "  \"\"\"  \n",
    "  \n",
    "  classes = []\n",
    "  with open(\"yolo/data/coco.names\", \"r\") as f: # read the coco dataset\n",
    "      classes = f.read().splitlines()  \n",
    "\n",
    "  # capture the height and width of every frame that we are going to use it scale back to the original image size\n",
    "  height, width, _ = img.shape  # Frame shape (1440, 2560, 3) \n",
    "\n",
    "  # creating a blob input (image, scaling, size of the image) Shape (1, 3, 416, 416)\n",
    "  blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "\n",
    "  # passing the blob into input function\n",
    "  network.setInput(blob)\n",
    "\n",
    "  # getting the output layers name ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "  output_layers_names = network.getUnconnectedOutLayersNames()\n",
    "\n",
    "  # getting the output layer list len 3 [0.9875224 , 0.99220854, 0.18105118, ..., 0. ,0.,0.]], dtype=float32)] \n",
    "  layerOutputs = network.forward(output_layers_names) \n",
    "\n",
    "  boxes = []\n",
    "  confidences = []\n",
    "  class_ids = [] # represent the predicted classes\n",
    "\n",
    "  detections = [] \n",
    "\n",
    "  for output in layerOutputs: # extract the information from each of the input\n",
    "      # print(type(output), output.shape) <class 'numpy.ndarray'> (507, 85) <class 'numpy.ndarray'> (2028, 85) <class 'numpy.ndarray'> (8112, 85)\n",
    "      \n",
    "      for detection in output: # extract the information from each of the output\n",
    "          det_data = []\n",
    "          scores = detection[5:]\n",
    "          class_id = np.argmax(scores) \n",
    "          confidence = scores[class_id]\n",
    "          # print(detection[0],detection[1]) 0.8738878 0.5129194\n",
    "\n",
    "          if confidence > 0.5:   # 0.5\n",
    "              center_x = int(detection[0]*width)\n",
    "              center_y = int(detection[1]*height) \n",
    "\n",
    "              w = int(detection[2]*width)\n",
    "              h = int(detection[3]*height)\n",
    "\n",
    "              x = int(center_x - w/2)\n",
    "              y = int(center_y - h/2)\n",
    "\n",
    "              class_name = str(classes[class_id])\n",
    "              if class_name == 'person':\n",
    "                  det_data.append(class_name)\n",
    "                  boxes.append([x, y, w, h])\n",
    "                  confidences.append((float(confidence)))\n",
    "                  class_ids.append(class_id) \n",
    "                  det_data.append(confidence)\n",
    "                  det_data.append([x,y,w,h])\n",
    "              # first 4 coeffcient is the location of the bounding box and the 5th element is the box confidence\n",
    "          if det_data:\n",
    "            detections.append(tuple(det_data)) \n",
    "          #else:\n",
    "          #  detections.append(np.empty((0,5)))\n",
    "      # (obj, score, [cx,cy,w,h])\n",
    "  return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_and_track(video_filename: str) -> Dict[str, List]:\n",
    "  \"\"\"\n",
    "  Detection and Tracking function based on YOLO-v3 object detector and kalman filter based SORT tracker.\n",
    "  Parameters\n",
    "    ----------\n",
    "    video_frames: path to the video file. Video would be a 4 dimesional np array of shape <N, C, H, W>.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    tracks: Dictionary of tracks where each key is the objectID and value is the list of the center of the\n",
    "    object on the floor plane.\n",
    "  \"\"\"\n",
    "\n",
    "  tracks = {}\n",
    "  person_tracker = sort.Sort()\n",
    "  # 1. Start reading the video file frame by frame\n",
    "  cap = cv2.VideoCapture(video_filename) \n",
    "\n",
    "  frameID = 0\n",
    "  while cap.isOpened():\n",
    "    frameID += 1\n",
    "\n",
    "    # 2. Iterate through each frame in the video\n",
    "    ret, frame = cap.read() \n",
    "\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "    \n",
    "    # If video end reached\n",
    "    if not ret:\n",
    "        break \n",
    "\n",
    "    # 3. Get the detections from the object detector\n",
    "    detections = detect(img)\n",
    "\n",
    "    # 4. Transform the detected points on floor plane from camera image plane\n",
    "    detections_on_floor_plane = []\n",
    "    dets = np.empty((0,5))  \n",
    "    for (obj, score, [cx,cy,w,h]) in detections:\n",
    "        #convert coordinates cx,cy,w,h to x1,y1,x2,y2. Project them onto floor plane and\n",
    "        # reorder the results to (bbox, score, object_name)\n",
    "        x1, y1, x2, y2 = get_corner_coordinates([cx, cy, w, h])\n",
    "        detection = np.array([x1, y1, x2, y2, score])   \n",
    "        dets = np.vstack((dets,detection)) \n",
    "\n",
    "        try:\n",
    "            # 5. Find association of the detected objects and add the objects into list of tracks Using SORT.\n",
    "            if detections is not None:\n",
    "                # 6. Update the tracks\n",
    "                tracked_persons = person_tracker.update(dets)\n",
    "\n",
    "                for x1, y1, x2, y2, personid in tracked_persons:\n",
    "                    # 7. For each tracked object, get the center pixel on the image plane and add it to the object trajectory.\n",
    "                    center_pos = (int((x1 + x2)/2), int(y1 + y2)/2)\n",
    "                    tracks[personid] = tracks.get(personid, []) + [center_pos]\n",
    "                    print('tracks', tracks)  \n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ac8a9166d262f8ae0d6186ed3a16fd1834d6370d5b1770fc315ffc7decd9810"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
