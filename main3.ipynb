{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sort\n",
    "from utils import *\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "YOLO-v3 based object detector. This YOLO-v3 is pretrained on MS-COCO dataset.\n",
    "\"\"\"\n",
    "network = cv2.dnn.readNet(\"yolo/weights/yolov3.weights\",\"yolo/cfg/yolov3.cfg\") #  \"yolo/cfg/coco.data\"\n",
    "\n",
    "\n",
    "def detect(img):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  img: PIL Input Image\n",
    "  category: category of the object to filter(should be one of the categories from MS-COCO dataset)\n",
    "\n",
    "  Returns\n",
    "  ---------- \n",
    "  detections: List of detections. Each detection is a tuple of form (object_name, score, bbox).\n",
    "  \"\"\"  \n",
    "  \n",
    "  classes = []\n",
    "  with open(\"yolo/data/coco.names\", \"r\") as f: # read the coco dataset\n",
    "      classes = f.read().splitlines()  \n",
    "\n",
    "  # capture the height and width of every frame that we are going to use it scale back to the original image size\n",
    "  height, width, _ = img.shape  # Frame shape (1440, 2560, 3) \n",
    "\n",
    "  # creating a blob input (image, scaling, size of the image) Shape (1, 3, 416, 416)\n",
    "  blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "\n",
    "  # passing the blob into input function\n",
    "  network.setInput(blob)\n",
    "\n",
    "  # getting the output layers name ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "  output_layers_names = network.getUnconnectedOutLayersNames()\n",
    "\n",
    "  # getting the output layer list len 3 [0.9875224 , 0.99220854, 0.18105118, ..., 0. ,0.,0.]], dtype=float32)] \n",
    "  layerOutputs = network.forward(output_layers_names) \n",
    "\n",
    "  boxes = []\n",
    "  confidences = []\n",
    "  class_ids = [] # represent the predicted classes\n",
    "\n",
    "  detections = [] \n",
    "\n",
    "  for output in layerOutputs: # extract the information from each of the input\n",
    "      # print(type(output), output.shape) <class 'numpy.ndarray'> (507, 85) <class 'numpy.ndarray'> (2028, 85) <class 'numpy.ndarray'> (8112, 85)\n",
    "      \n",
    "      for detection in output: # extract the information from each of the output\n",
    "          det_data = []\n",
    "          scores = detection[5:]\n",
    "          class_id = np.argmax(scores) \n",
    "          confidence = scores[class_id]\n",
    "          # print(detection[0],detection[1]) 0.8738878 0.5129194\n",
    "\n",
    "          if confidence > 0.5:   # 0.5\n",
    "              center_x = int(detection[0]*width)\n",
    "              center_y = int(detection[1]*height) \n",
    "\n",
    "              w = int(detection[2]*width)\n",
    "              h = int(detection[3]*height)\n",
    "\n",
    "              x = int(center_x - w/2)\n",
    "              y = int(center_y - h/2)\n",
    "\n",
    "              class_name = str(classes[class_id])\n",
    "              if class_name == 'person':\n",
    "                  det_data.append(class_name)\n",
    "                  boxes.append([x, y, w, h])\n",
    "                  confidences.append((float(confidence)))\n",
    "                  class_ids.append(class_id) \n",
    "                  det_data.append(confidence)\n",
    "                  det_data.append([x,y,w,h])\n",
    "              # first 4 coeffcient is the location of the bounding box and the 5th element is the box confidence\n",
    "          if det_data:\n",
    "            detections.append(tuple(det_data)) \n",
    "          #else:\n",
    "          #  detections.append(np.empty((0,5)))\n",
    "      # (obj, score, [cx,cy,w,h])\n",
    "  return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_and_track(video_filename: str) -> Dict[str, List]:\n",
    "  \"\"\"\n",
    "  Detection and Tracking function based on YOLO-v3 object detector and kalman filter based SORT tracker.\n",
    "  Parameters\n",
    "    ----------\n",
    "    video_frames: path to the video file. Video would be a 4 dimesional np array of shape <N, C, H, W>.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    tracks: Dictionary of tracks where each key is the objectID and value is the list of the center of the\n",
    "    object on the floor plane.\n",
    "  \"\"\"\n",
    "\n",
    "  data_input = np.array([0,0,0,0])\n",
    "  tracks = {}\n",
    "  person_tracker = sort.Sort() \n",
    "\n",
    "  # 1. Start reading the video file frame by frame\n",
    "  cap = cv2.VideoCapture(video_filename) \n",
    "\n",
    "  frameID = 0\n",
    "  while cap.isOpened():\n",
    "    frameID += 1\n",
    "    try:\n",
    "      # 2. Iterate through each frame in the video\n",
    "      ret, frame = cap.read() \n",
    "\n",
    "      img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "      \n",
    "      # If video end reached\n",
    "      if not ret:\n",
    "          break \n",
    "\n",
    "      # 3. Get the detections from the object detector\n",
    "      detections = detect(img)\n",
    "\n",
    "      # 4. Transform the detected points on floor plane from camera image plane\n",
    "      detections_on_floor_plane = []\n",
    "      dets = np.empty((0,5))  \n",
    "      for (obj, score, [cx,cy,w,h]) in detections:\n",
    "          #convert coordinates cx,cy,w,h to x1,y1,x2,y2. Project them onto floor plane and\n",
    "          # reorder the results to (bbox, score, object_name)\n",
    "          x1, y1, x2, y2 = get_corner_coordinates([cx, cy, w, h])\n",
    "          detection = np.array([x1, y1, x2, y2, score])   \n",
    "          dets = np.vstack((dets,detection)) \n",
    "\n",
    "      try:\n",
    "          # 5. Find association of the detected objects and add the objects into list of tracks Using SORT.\n",
    "          if detections is not None:\n",
    "            # 6. Update the tracks\n",
    "            tracked_persons = person_tracker.update(dets)\n",
    "\n",
    "            for x1, y1, x2, y2, personid in tracked_persons:\n",
    "              # 7. For each tracked object, get the center pixel on the image plane and add it to the object trajectory.\n",
    "              center_pos = (int((x1 + x2)/2), int(y1 + y2)/2)\n",
    "              tracks[personid] = tracks.get(personid, []) + [center_pos]\n",
    "              \n",
    "              for each_record in tracks[personid]:\n",
    "                record = np.array([frameID, personid, each_record[0], each_record[1] ]) \n",
    "                data_input=np.vstack((data_input, record)) \n",
    "\n",
    "      except Exception as e:\n",
    "          print(e) \n",
    "          break\n",
    "    except Exception as ex:\n",
    "      print(ex)\n",
    "      break \n",
    "  return tracks, data_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wvn_it83\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  video_path= 'Videos/cam3_004.mp4'\n",
    "  tracks, data_input = detect_and_track(video_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23621, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data_input.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_pos_raw = pd.DataFrame(data_input)\n",
    "pixel_pos = pixel_pos_raw.T \n",
    "pixel_pos = pixel_pos.iloc[:,1:]  \n",
    "pixel_pos.to_csv('pixel_pos_pixels.csv', index=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_inverse_pespective(perspective_matrix: np.array)-> np.array:\n",
    "  \"\"\"\n",
    "  This method calculates the inverse of prespective matrix by homography. \n",
    "  - Takes 4 random points on the floor plane(destination_plane) and calculates the corresponding points \n",
    "  on the camera image plane(src_plane) using perspective matrix.\n",
    "  - Calculates the Homography matrix to map any point in image plane to floor plane.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  perspective_matrix: 3 x 4 camera prespective matrix to convert 3d homogeneous world coordinates to \n",
    "  2d homogeneous camera coordinates.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  3x3 homography matrix for moving from 2d homogeneous image plane to world floor plane(at z=0)\n",
    "  \"\"\"\n",
    "  \n",
    "  #Take 5 homogenous points on the floor(Unit is in Meters)\n",
    "  pts_dst = np.array([[0,0,0,1],\n",
    "                      [0,1,0,1],\n",
    "                      [1,0,0,1],\n",
    "                      [1,1,0,1],\n",
    "                      [0,0,0,1]\n",
    "                    ])\n",
    "  #Obtain respective homogenous points on the image plane\n",
    "  pts_src = (perspective_matrix @ pts_dst.T).T\n",
    "  \n",
    "  #convert homogenous coordinates to cartesian coordinates\n",
    "  pts_src_cart = np.array([[x/w, y/w] for x,y,w in pts_src])\n",
    "  pts_dst_cart = np.array([[x/w, y/w] for x,y,z,w in pts_dst])\n",
    "  \n",
    "  #find the 3x3 Homography Matrix for transforming image plane to floor plane\n",
    "  h, status = cv2.findHomography(pts_src_cart, pts_dst_cart)\n",
    "  return h\n",
    "\n",
    "\n",
    "def project_to_floor(image_coordinates: List[int], h: np.array) -> List[int]: \n",
    "  \"\"\"\n",
    "  This method takes the Homography matrix and the 2d image cartesian coordinates. It returns the (x, y)\n",
    "  cartesian coordinates in 3d cartesian world coordinates on floor plane(at z=0). Notice that z coordinate is omitted\n",
    "  here and added inside the tracking function. \n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  image_coordinates: 2d pixel coordinates (x,y)\n",
    "  h: 3x3 Homography matrix np.array[3x3]\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  floor_coordinates: List of x, y coordinates in 3d world of same pixel on floor plane i.e. (x,y,z) Considering z=0 and \n",
    "  ommitted here.\n",
    "  \"\"\"\n",
    "  #adding 1 for homogenous coordinate system\n",
    "  x, y, w = h@np.array([[*image_coordinates, 1]]).T\n",
    "  return [x/w, y/w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1667, 174.5)\n",
      "[array([0.99110929]), array([0.39999998])]\n",
      "(1668, 173.5)\n",
      "[array([0.99110929]), array([0.39999998])]\n",
      "(1668, 173.0)\n",
      "[array([0.99110929]), array([0.39999998])]\n",
      "(1668, 173.0)\n",
      "[array([0.99110929]), array([0.39999998])]\n",
      "(1668, 174.5)\n",
      "[array([0.99110929]), array([0.39999998])]\n",
      "(1666, 174.5)\n",
      "[array([0.99110929]), array([0.39999998])]\n"
     ]
    }
   ],
   "source": [
    "# Dummy perspective matrix \n",
    "dummy_perspective_matrix = np.array([[1.5,0.5,1.5,0.5],[0.5,1.5,0.5,1.5],[0.5,0.5,1.5,0.5]], np.float32)\n",
    "\n",
    "h= get_inverse_pespective(dummy_perspective_matrix) \n",
    "for point in tracks[1625.0]:\n",
    "    print(point) \n",
    "    print(project_to_floor(point,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ac8a9166d262f8ae0d6186ed3a16fd1834d6370d5b1770fc315ffc7decd9810"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
