{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sort\n",
    "from utils import *\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "YOLO-v3 based object detector. This YOLO-v3 is pretrained on MS-COCO dataset.\n",
    "\"\"\"\n",
    "network = cv2.dnn.readNet(\"yolo/weights/yolov3.weights\",\"yolo/cfg/yolov3.cfg\") #  \"yolo/cfg/coco.data\"\n",
    "\n",
    "\n",
    "def detect(img):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  img: PIL Input Image\n",
    "  category: category of the object to filter(should be one of the categories from MS-COCO dataset)\n",
    "\n",
    "  Returns\n",
    "  ---------- \n",
    "  detections: List of detections. Each detection is a tuple of form (object_name, score, bbox).\n",
    "  \"\"\"  \n",
    "  \n",
    "  classes = []\n",
    "  with open(\"yolo/data/coco.names\", \"r\") as f: # read the coco dataset\n",
    "      classes = f.read().splitlines()  \n",
    "\n",
    "  # capture the height and width of every frame that we are going to use it scale back to the original image size\n",
    "  height, width, _ = img.shape  # Frame shape (1440, 2560, 3) \n",
    "\n",
    "  # creating a blob input (image, scaling, size of the image) Shape (1, 3, 416, 416)\n",
    "  blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), (0,0,0), swapRB=True, crop=False)\n",
    "\n",
    "  # passing the blob into input function\n",
    "  network.setInput(blob)\n",
    "\n",
    "  # getting the output layers name ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "  output_layers_names = network.getUnconnectedOutLayersNames()\n",
    "\n",
    "  # getting the output layer list len 3 [0.9875224 , 0.99220854, 0.18105118, ..., 0. ,0.,0.]], dtype=float32)] \n",
    "  layerOutputs = network.forward(output_layers_names) \n",
    "\n",
    "  boxes = []\n",
    "  confidences = []\n",
    "  class_ids = [] # represent the predicted classes\n",
    "\n",
    "  detections = [] \n",
    "\n",
    "  for output in layerOutputs: # extract the information from each of the input\n",
    "      # print(type(output), output.shape) <class 'numpy.ndarray'> (507, 85) <class 'numpy.ndarray'> (2028, 85) <class 'numpy.ndarray'> (8112, 85)\n",
    "      \n",
    "      for detection in output: # extract the information from each of the output\n",
    "          det_data = []\n",
    "          scores = detection[5:]\n",
    "          class_id = np.argmax(scores) \n",
    "          confidence = scores[class_id]\n",
    "          # print(detection[0],detection[1]) 0.8738878 0.5129194\n",
    "\n",
    "          if confidence > 0.5:   # 0.5\n",
    "              center_x = int(detection[0]*width)\n",
    "              center_y = int(detection[1]*height) \n",
    "\n",
    "              w = int(detection[2]*width)\n",
    "              h = int(detection[3]*height)\n",
    "\n",
    "              x = int(center_x - w/2)\n",
    "              y = int(center_y - h/2)\n",
    "\n",
    "              class_name = str(classes[class_id])\n",
    "              if class_name == 'person':\n",
    "                  det_data.append(class_name)\n",
    "                  boxes.append([x, y, w, h])\n",
    "                  confidences.append((float(confidence)))\n",
    "                  class_ids.append(class_id) \n",
    "                  det_data.append(confidence)\n",
    "                  det_data.append([x,y,w,h])\n",
    "              # first 4 coeffcient is the location of the bounding box and the 5th element is the box confidence\n",
    "          if det_data:\n",
    "            detections.append(tuple(det_data)) \n",
    "          #else:\n",
    "          #  detections.append(np.empty((0,5)))\n",
    "      # (obj, score, [cx,cy,w,h])\n",
    "  return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_and_track(video_filename: str) -> Dict[str, List]:\n",
    "  \"\"\"\n",
    "  Detection and Tracking function based on YOLO-v3 object detector and kalman filter based SORT tracker.\n",
    "  Parameters\n",
    "    ----------\n",
    "    video_frames: path to the video file. Video would be a 4 dimesional np array of shape <N, C, H, W>.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    tracks: Dictionary of tracks where each key is the objectID and value is the list of the center of the\n",
    "    object on the floor plane.\n",
    "  \"\"\"\n",
    "\n",
    "  data_input = np.array([0,0,0,0])\n",
    "  tracks = {}\n",
    "  person_tracker = sort.Sort() \n",
    "\n",
    "  # 1. Start reading the video file frame by frame\n",
    "  cap = cv2.VideoCapture(video_filename) \n",
    "\n",
    "  frameID = 0\n",
    "  while cap.isOpened():\n",
    "    frameID += 1\n",
    "    try:\n",
    "      # 2. Iterate through each frame in the video\n",
    "      ret, frame = cap.read() \n",
    "\n",
    "      img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "      \n",
    "      # If video end reached\n",
    "      if not ret:\n",
    "          break \n",
    "\n",
    "      # 3. Get the detections from the object detector\n",
    "      detections = detect(img)\n",
    "\n",
    "      # 4. Transform the detected points on floor plane from camera image plane\n",
    "      detections_on_floor_plane = []\n",
    "      dets = np.empty((0,5))  \n",
    "      for (obj, score, [cx,cy,w,h]) in detections:\n",
    "          #convert coordinates cx,cy,w,h to x1,y1,x2,y2. Project them onto floor plane and\n",
    "          # reorder the results to (bbox, score, object_name)\n",
    "          x1, y1, x2, y2 = get_corner_coordinates([cx, cy, w, h])\n",
    "          detection = np.array([x1, y1, x2, y2, score])   \n",
    "          dets = np.vstack((dets,detection)) \n",
    "\n",
    "      try:\n",
    "          # 5. Find association of the detected objects and add the objects into list of tracks Using SORT.\n",
    "          if detections is not None:\n",
    "            # 6. Update the tracks\n",
    "            tracked_persons = person_tracker.update(dets)\n",
    "\n",
    "            for x1, y1, x2, y2, personid in tracked_persons:\n",
    "              # 7. For each tracked object, get the center pixel on the image plane and add it to the object trajectory.\n",
    "              center_pos = (int((x1 + x2)/2), int(y1 + y2)/2)\n",
    "              tracks[personid] = tracks.get(personid, []) + [center_pos]\n",
    "              \n",
    "              for each_record in tracks[personid]:\n",
    "                record = np.array([frameID, personid, each_record[0], each_record[1] ]) \n",
    "                data_input=np.vstack((data_input, record)) \n",
    "\n",
    "      except Exception as e:\n",
    "          print(e) \n",
    "          break\n",
    "    except Exception as ex:\n",
    "      print(ex)\n",
    "      break \n",
    "  return tracks, data_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wvn_it83\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  video_path= 'Videos/cam3_004.mp4'\n",
    "  tracks, data_input = detect_and_track(video_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23621, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data_input.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_pos_raw = pd.DataFrame(data_input)\n",
    "pixel_pos = pixel_pos_raw.T \n",
    "pixel_pos = pixel_pos.iloc[:,1:]  \n",
    "pixel_pos.to_csv('pixel_pos_pixels.csv', index=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_inverse_pespective(perspective_matrix: np.array)-> np.array:\n",
    "  \"\"\"\n",
    "  This method calculates the inverse of prespective matrix by homography. \n",
    "  - Takes 4 random points on the floor plane(destination_plane) and calculates the corresponding points \n",
    "  on the camera image plane(src_plane) using perspective matrix.\n",
    "  - Calculates the Homography matrix to map any point in image plane to floor plane.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  perspective_matrix: 3 x 4 camera prespective matrix to convert 3d homogeneous world coordinates to \n",
    "  2d homogeneous camera coordinates.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  3x3 homography matrix for moving from 2d homogeneous image plane to world floor plane(at z=0)\n",
    "  \"\"\"\n",
    "  \n",
    "  #Take 5 homogenous points on the floor(Unit is in Meters)\n",
    "  pts_dst = np.array([[0,0,0,1],\n",
    "                      [0,1,0,1],\n",
    "                      [1,0,0,1],\n",
    "                      [1,1,0,1],\n",
    "                      [0,0,0,1]\n",
    "                    ])\n",
    "  #Obtain respective homogenous points on the image plane\n",
    "  pts_src = (perspective_matrix @ pts_dst.T).T\n",
    "  \n",
    "  #convert homogenous coordinates to cartesian coordinates\n",
    "  pts_src_cart = np.array([[x/w, y/w] for x,y,w in pts_src])\n",
    "  pts_dst_cart = np.array([[x/w, y/w] for x,y,z,w in pts_dst])\n",
    "  \n",
    "  #find the 3x3 Homography Matrix for transforming image plane to floor plane\n",
    "  h, status = cv2.findHomography(pts_src_cart, pts_dst_cart)\n",
    "  return h\n",
    "\n",
    "\n",
    "def project_to_floor(image_coordinates: List[int], h: np.array) -> List[int]: \n",
    "  \"\"\"\n",
    "  This method takes the Homography matrix and the 2d image cartesian coordinates. It returns the (x, y)\n",
    "  cartesian coordinates in 3d cartesian world coordinates on floor plane(at z=0). Notice that z coordinate is omitted\n",
    "  here and added inside the tracking function. \n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  image_coordinates: 2d pixel coordinates (x,y)\n",
    "  h: 3x3 Homography matrix np.array[3x3]\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  floor_coordinates: List of x, y coordinates in 3d world of same pixel on floor plane i.e. (x,y,z) Considering z=0 and \n",
    "  ommitted here.\n",
    "  \"\"\"\n",
    "  #adding 1 for homogenous coordinate system\n",
    "  x, y, w = h@np.array([[*image_coordinates, 1]]).T\n",
    "  return [x/w, y/w]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-transform-a-2d-image-into-a-3d-space-5fc2306e3d36\n",
    "\n",
    "def transform(video_filename,\n",
    "              translation=(0, 0, 0),\n",
    "              rotation=(0, 0, 0),\n",
    "              scaling=(1, 1, 1),\n",
    "              shearing=(0, 0, 0)):\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_filename) \n",
    "    _, frame = cap.read()  \n",
    "\n",
    "\n",
    "    # get the values on each axis\n",
    "    t_x, t_y, t_z = translation\n",
    "    r_x, r_y, r_z = rotation\n",
    "    sc_x, sc_y, sc_z = scaling\n",
    "    sh_x, sh_y, sh_z = shearing\n",
    "    \n",
    "    # convert degree angles to rad\n",
    "    theta_rx = np.deg2rad(r_x)\n",
    "    theta_ry = np.deg2rad(r_y)\n",
    "    theta_rz = np.deg2rad(r_z)\n",
    "    theta_shx = np.deg2rad(sh_x)\n",
    "    theta_shy = np.deg2rad(sh_y)\n",
    "    theta_shz = np.deg2rad(sh_z)\n",
    "    \n",
    "    # get the height and the width of the image\n",
    "    h, w = frame.shape[:2]\n",
    "    # compute its diagonal\n",
    "    diag = (h ** 2 + w ** 2) ** 0.5\n",
    "    # compute the focal length\n",
    "    f = diag\n",
    "    if np.sin(theta_rz) != 0:\n",
    "        f /= 2 * np.sin(theta_rz)\n",
    "        \n",
    "    # set the image from cartesian to projective dimension\n",
    "    H_M = np.array([[1, 0, -w / 2],\n",
    "                    [0, 1, -h / 2],\n",
    "                    [0, 0,      1],\n",
    "                    [0, 0,      1]])\n",
    "    # set the image projective to carrtesian dimension\n",
    "    Hp_M = np.array([[f, 0, w / 2, 0],\n",
    "                     [0, f, h / 2, 0],\n",
    "                     [0, 0,     1, 0]])\n",
    "    \"\"\"\n",
    "            We will define our matrices here in next parts\n",
    "                                                            \"\"\"\n",
    "    Identity = np.array([[1, 0, 0, 0],\n",
    "                         [0, 1, 0, 0],\n",
    "                         [0, 0, 1, 0],\n",
    "                         [0, 0, 0, 1]])\n",
    "    \n",
    "    # adjust the translation on z\n",
    "    t_z = (f - t_z) / sc_z ** 2\n",
    "    # translation matrix to translate the image\n",
    "    T_M = np.array([[1, 0, 0, t_x],\n",
    "                    [0, 1, 0, t_y],\n",
    "                    [0, 0, 1, t_z],\n",
    "                    [0, 0, 0,  1]])\n",
    "\n",
    "        # calculate cos and sin of angles\n",
    "    sin_rx, cos_rx = np.sin(theta_rx), np.cos(theta_rx)\n",
    "    sin_ry, cos_ry = np.sin(theta_ry), np.cos(theta_ry)\n",
    "    sin_rz, cos_rz = np.sin(theta_rz), np.cos(theta_rz)\n",
    "    # get the rotation matrix on x axis\n",
    "    R_Mx = np.array([[1,      0,       0, 0],\n",
    "                     [0, cos_rx, -sin_rx, 0],\n",
    "                     [0, sin_rx,  cos_rx, 0],\n",
    "                     [0,      0,       0, 1]])\n",
    "    # get the rotation matrix on y axis\n",
    "    R_My = np.array([[cos_ry, 0, -sin_ry, 0],\n",
    "                     [     0, 1,       0, 0],\n",
    "                     [sin_ry, 0,  cos_ry, 0],\n",
    "                     [     0, 0,       0, 1]])\n",
    "    # get the rotation matrix on z axis\n",
    "    R_Mz = np.array([[cos_rz, -sin_rz, 0, 0],\n",
    "                     [sin_rz,  cos_rz, 0, 0],\n",
    "                     [     0,       0, 1, 0],\n",
    "                     [     0,       0, 0, 1]])\n",
    "    # compute the full rotation matrix\n",
    "    R_M = np.dot(np.dot(R_Mx, R_My), R_Mz)\n",
    "\n",
    "\n",
    "    # get the scaling matrix\n",
    "    Sc_M = np.array([[sc_x,     0,    0, 0],\n",
    "                     [   0,  sc_y,    0, 0],\n",
    "                     [   0,     0, sc_z, 0],\n",
    "                     [   0,     0,    0, 1]])\n",
    "\n",
    "    # get the tan of angles\n",
    "    tan_shx = np.tan(theta_shx)\n",
    "    tan_shy = np.tan(theta_shy)\n",
    "    tan_shz = np.tan(theta_shz)\n",
    "    # get the shearing matrix on x axis\n",
    "    Sh_Mx = np.array([[      1, 0, 0, 0],\n",
    "                      [tan_shy, 1, 0, 0],\n",
    "                      [tan_shz, 0, 1, 0],\n",
    "                      [      0, 0, 0, 1]])\n",
    "    # get the shearing matrix on y axis\n",
    "    Sh_My = np.array([[1, tan_shx, 0, 0],\n",
    "                      [0,       1, 0, 0],\n",
    "                      [0, tan_shz, 1, 0],\n",
    "                      [0,       0, 0, 1]])\n",
    "    # get the shearing matrix on z axis\n",
    "    Sh_Mz = np.array([[1, 0, tan_shx, 0],\n",
    "                      [0, 1, tan_shy, 0],\n",
    "                      [0, 0,       1, 0],\n",
    "                      [0, 0,       0, 1]])\n",
    "    # compute the full shearing matrix\n",
    "    Sh_M = np.dot(np.dot(Sh_Mx, Sh_My), Sh_Mz)\n",
    "\n",
    "\n",
    "    # compute the full transform matrix\n",
    "    M = Identity\n",
    "    #M = np.dot(T_M,  M)\n",
    "    #M = np.dot(R_M,  M)\n",
    "    #M = np.dot(Sc_M, M)\n",
    "    #M = np.dot(Sh_M, M)\n",
    "    M = np.dot(Hp_M, np.dot(M, H_M))\n",
    "    # apply the transformation\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\work\\masters\\Trento\\Academics\\Semesters\\Year_1_Semester_2\\140266_Computer_Vision\\project\\KalmanTracker\\main3.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/work/masters/Trento/Academics/Semesters/Year_1_Semester_2/140266_Computer_Vision/project/KalmanTracker/main3.ipynb#ch0000007?line=1'>2</a>\u001b[0m v_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mVideos/cam3_004.mp4\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/work/masters/Trento/Academics/Semesters/Year_1_Semester_2/140266_Computer_Vision/project/KalmanTracker/main3.ipynb#ch0000007?line=2'>3</a>\u001b[0m perspective_matrix \u001b[39m=\u001b[39m transform(v_path)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/work/masters/Trento/Academics/Semesters/Year_1_Semester_2/140266_Computer_Vision/project/KalmanTracker/main3.ipynb#ch0000007?line=4'>5</a>\u001b[0m dummy_perspective_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m2937\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m3758348\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2937\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2114071\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]], np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/work/masters/Trento/Academics/Semesters/Year_1_Semester_2/140266_Computer_Vision/project/KalmanTracker/main3.ipynb#ch0000007?line=6'>7</a>\u001b[0m h\u001b[39m=\u001b[39m get_inverse_pespective(perspective_matrix) \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/work/masters/Trento/Academics/Semesters/Year_1_Semester_2/140266_Computer_Vision/project/KalmanTracker/main3.ipynb#ch0000007?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m point \u001b[39min\u001b[39;00m tracks[\u001b[39m1625.0\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Dummy perspective matrix \n",
    "v_path = 'Videos/cam3_004.mp4'\n",
    "perspective_matrix = transform(v_path)\n",
    "\n",
    "dummy_perspective_matrix = np.array([[2937,0,-3758348,0],[0,-2937,-2114071,0],[0,0,1,0]], np.float32)\n",
    "\n",
    "h= get_inverse_pespective(perspective_matrix) \n",
    "for point in tracks[1625.0]:\n",
    "    print(point) \n",
    "    print(project_to_floor(point,h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ac8a9166d262f8ae0d6186ed3a16fd1834d6370d5b1770fc315ffc7decd9810"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
